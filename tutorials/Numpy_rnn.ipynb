{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global config variables\n",
    "num_steps = 5 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 200\n",
    "num_classes = 2\n",
    "state_size = 4\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        生成x,y， 关于x，y的依赖，x是随机生成的0和1的数组，y也是50%的base chance生成1，50%base chance 生成0。 存在依赖，the chance of yt=1 is increase 50% if x(t-3)=1, and decrease 25% if x(t-8)=1, 所以，如果x(t-3),x(t-8) 都为1，yt=1的概率是50 + 50 - 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(size=1000000):\n",
    "    X = np.array(np.random.choice(2, size=(size,)))\n",
    "    Y = []\n",
    "    for i in range(size):\n",
    "        threshold = 0.5\n",
    "        if X[i-3] == 1:\n",
    "            threshold += 0.5\n",
    "        if X[i-8] == 1:\n",
    "            threshold -= 0.25\n",
    "        if np.random.rand() > threshold:\n",
    "            Y.append(0)\n",
    "        else:\n",
    "            Y.append(1)\n",
    "    return X, np.array(Y)\n",
    "\n",
    "# adapted from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/reader.py\n",
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "    raw_x, raw_y = raw_data\n",
    "    data_length = len(raw_x)\n",
    "\n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    batch_partition_length = data_length // batch_size\n",
    "    data_x = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    for i in range(batch_size):\n",
    "        data_x[i] = raw_x[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_partition_length * i:batch_partition_length * (i + 1)]\n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    epoch_size = batch_partition_length // num_steps\n",
    "\n",
    "    for i in range(epoch_size):\n",
    "        x = data_x[:, i * num_steps:(i + 1) * num_steps]\n",
    "        y = data_y[:, i * num_steps:(i + 1) * num_steps]\n",
    "        yield (x, y)\n",
    "\n",
    "def gen_epochs(n, num_steps):\n",
    "    for i in range(n):\n",
    "        yield gen_batch(gen_data(), batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "RNN Inputs\n",
    "\"\"\"\n",
    "\n",
    "# Turn our x placeholder into a list of one-hot tensors:\n",
    "# rnn_inputs is a list of num_steps tensors with shape [batch_size, num_classes]\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.variable_scope('rnn_cell'):\n",
    "#     W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "#     b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "def rnn_cell(rnn_input, state):\n",
    "    with tf.variable_scope('rnn_cell', reuse=True):\n",
    "        W = tf.get_variable('W', [num_classes + state_size, state_size])\n",
    "        b = tf.get_variable('b', [state_size], initializer=tf.constant_initializer(0.0))\n",
    "    return tf.tanh(tf.matmul(tf.concat([rnn_input, state], 1), W) + b)\n",
    "\n",
    "state = init_state\n",
    "rnn_outputs = []\n",
    "for rnn_input in rnn_inputs:\n",
    "    state = rnn_cell(rnn_input, state)\n",
    "    rnn_outputs.append(state)\n",
    "final_state = rnn_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the forward can be replace by\n",
    "# cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "# rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "# Turn our y placeholder into a list of labels\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "#losses and train_step\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 0\n",
      "Average loss at step 100 for last 250 steps: 0.645741816759\n",
      "Average loss at step 200 for last 250 steps: 0.626104823351\n",
      "Average loss at step 300 for last 250 steps: 0.599491884112\n",
      "Average loss at step 400 for last 250 steps: 0.540761834681\n",
      "Average loss at step 500 for last 250 steps: 0.521645544469\n",
      "Average loss at step 600 for last 250 steps: 0.522875803411\n",
      "Average loss at step 700 for last 250 steps: 0.521338348389\n",
      "Average loss at step 800 for last 250 steps: 0.519605456591\n",
      "Average loss at step 900 for last 250 steps: 0.519520724416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11593ce80>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQTzP/9WPLxmmEEjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1019c07b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_losses = train_network(1,num_steps)\n",
    "plt.plot(training_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    ##Final model — static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "Inputs\n",
    "\"\"\"\n",
    "\n",
    "x_one_hot = tf.one_hot(x, num_classes)\n",
    "rnn_inputs = tf.unstack(x_one_hot, axis=1)\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_output, W) + b for rnn_output in rnn_outputs]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = tf.unstack(y, num=num_steps, axis=1)\n",
    "\n",
    "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=logit) for \\\n",
    "          logit, label in zip(logits, y_as_list)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    ##Final model — dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-ceef248e8fe2>\", line 2, in <module>\n    W = tf.get_variable('W', [state_size, num_classes])\n  File \"/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5f1942b65aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m logits = tf.reshape(\n",
      "\u001b[0;32m/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    986\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 988\u001b[0;31m       custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    989\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m    990\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m           custom_getter=custom_getter)\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, custom_getter)\u001b[0m\n\u001b[1;32m    346\u001b[0m           \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m           validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape)\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m           caching_device=caching_device, validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape)\u001b[0m\n\u001b[1;32m    637\u001b[0m                          \u001b[0;34m\" Did you mean to set reuse=True in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 639\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    640\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable softmax/W already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"<ipython-input-12-ceef248e8fe2>\", line 2, in <module>\n    W = tf.get_variable('W', [state_size, num_classes])\n  File \"/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/qiuboqiao/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Placeholders\n",
    "\"\"\"\n",
    "\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size, state_size])\n",
    "\n",
    "\"\"\"\n",
    "Inputs\n",
    "\"\"\"\n",
    "\n",
    "rnn_inputs = tf.one_hot(x, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "RNN\n",
    "\"\"\"\n",
    "\n",
    "cell = tf.contrib.rnn.BasicRNNCell(state_size)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "\"\"\"\n",
    "Predictions, loss, training step\n",
    "\"\"\"\n",
    "\n",
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = tf.reshape(\n",
    "            tf.matmul(tf.reshape(rnn_outputs, [-1, state_size]), W) + b,\n",
    "            [batch_size, num_steps, num_classes])\n",
    "predictions = tf.nn.softmax(logits)\n",
    "\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
